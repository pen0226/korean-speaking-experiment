"""
data_io.py
ì‹¤í—˜ ë°ì´í„° ì €ì¥, ë°±ì—…, ì—…ë¡œë“œ ë° ë¡œê·¸ ê´€ë¦¬ ëª¨ë“ˆ (TOPIK ì—‘ì…€ ZIP í¬í•¨ ë³´ì¥)
GCS ë§¤í•‘ íŒŒì¼ ë™ê¸°í™” ìµœì í™” ì ìš©
"""

import os
import csv
import json
import zipfile
import pandas as pd
from datetime import datetime, timedelta
import streamlit as st
from config import (
    FOLDERS, 
    DATA_RETENTION_DAYS, 
    EXPERIMENT_QUESTION,
    GCS_ENABLED,
    GCS_BUCKET_NAME,
    GCS_SERVICE_ACCOUNT,
    GCS_SIMPLE_STRUCTURE,
    LOG_FORMAT,
    CURRENT_SESSION,
    SESSION_LABELS
)

def extract_task_completion_check(detailed_feedback):
    """
    detailed_feedbackì—ì„œ Task Completion Check ì •ë³´ ì¶”ì¶œ
    
    Args:
        detailed_feedback: GPTê°€ ìƒì„±í•œ detailed_feedback í…ìŠ¤íŠ¸
        
    Returns:
        dict: Task completion ì •ë³´
    """
    result = {
        'past_vacation_status': 'Unknown',
        'future_plans_status': 'Unknown',
        'tense_usage': 'Unknown',
        'both_topics_covered': False,
        'missing_topics': []
    }
    
    if not detailed_feedback:
        return result
    
    # Task Completion Check ì„¹ì…˜ ì°¾ê¸°
    if 'ğŸš© Task Completion Check' in detailed_feedback:
        lines = detailed_feedback.split('\n')
        for line in lines:
            line = line.strip()
            
            # Past vacation ì²´í¬
            if 'Past vacation:' in line:
                if 'âœ…' in line:
                    if 'Covered well' in line:
                        result['past_vacation_status'] = 'Covered'
                    elif 'Partially' in line:
                        result['past_vacation_status'] = 'Partially'
                elif 'âŒ' in line:
                    result['past_vacation_status'] = 'Missing'
                    result['missing_topics'].append('past_vacation')
            
            # Future plans ì²´í¬
            elif 'Future plans:' in line:
                if 'âœ…' in line:
                    if 'Covered well' in line:
                        result['future_plans_status'] = 'Covered'
                    elif 'Partially' in line:
                        result['future_plans_status'] = 'Partially'
                elif 'âŒ' in line:
                    result['future_plans_status'] = 'Missing'
                    result['missing_topics'].append('future_plans')
            
            # Tense usage ì²´í¬
            elif 'Tense usage:' in line:
                result['tense_usage'] = line.replace('âš ï¸', '').replace('Tense usage:', '').strip()
    
    # ë‘ ì£¼ì œ ëª¨ë‘ ë‹¤ë¤˜ëŠ”ì§€ íŒë‹¨
    result['both_topics_covered'] = (
        result['past_vacation_status'] in ['Covered', 'Partially'] and
        result['future_plans_status'] in ['Covered', 'Partially']
    )
    
    return result

def save_session_data():
    """
    ì„¸ì…˜ ë°ì´í„°ë¥¼ CSVë¡œ ì €ì¥ + ì°¸ê³ ìš© ì—‘ì…€ íŒŒì¼ ê²½ë¡œ ë°˜í™˜
    
    Returns:
        tuple: (csv_filename, reference_excel_filename, audio_folder, saved_files, zip_filename, timestamp)
    """
    try:
        # ì¤‘ë³µ ì €ì¥ ë°©ì§€
        if hasattr(st.session_state, 'data_saved') and st.session_state.data_saved:
            if hasattr(st.session_state, 'saved_files'):
                st.info("â„¹ï¸ Data already saved, using existing files.")
                existing_timestamp = getattr(st.session_state, 'saved_timestamp', datetime.now().strftime("%Y%m%d_%H%M%S"))
                
                # ğŸ”¥ ê¸°ì¡´ ì €ì¥ëœ íŒŒì¼ë“¤ì— reference ì—‘ì…€ ì¶”ê°€
                from save_reference_score import get_latest_reference_file
                reference_excel = get_latest_reference_file(existing_timestamp)
                
                # ê¸°ì¡´ saved_filesì— reference_excel ì¶”ê°€í•´ì„œ ë°˜í™˜
                saved_files = st.session_state.saved_files
                return saved_files[0], reference_excel, saved_files[2], saved_files[3], saved_files[4], existing_timestamp
        
        # í•„ìš”í•œ í´ë” ìƒì„±
        for folder in FOLDERS.values():
            os.makedirs(folder, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        session_data = build_session_data(timestamp)
        csv_filename = save_to_csv(session_data, timestamp)
        
        audio_folder, saved_files = save_audio_files(timestamp)
        
        # ğŸ”¥ ì°¸ê³ ìš© ì—‘ì…€ íŒŒì¼ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸° (ZIP ìƒì„± ì „ì— í™•ì¸)
        from save_reference_score import get_latest_reference_file
        reference_excel_filename = get_latest_reference_file(timestamp)
        
        # ğŸ”¥ ì—‘ì…€ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸ í›„ ZIP ìƒì„±
        if reference_excel_filename and os.path.exists(reference_excel_filename):
            print(f"âœ… Reference Excel found before ZIP creation: {reference_excel_filename}")
        else:
            print(f"âš ï¸ Reference Excel not found before ZIP creation: {reference_excel_filename}")
        
        zip_filename = create_comprehensive_backup_zip(st.session_state.session_id, timestamp, reference_excel_filename)
        
        return csv_filename, reference_excel_filename, audio_folder, saved_files, zip_filename, timestamp
    
    except Exception as e:
        st.error(f"âŒ Error saving session data: {str(e)}")
        return None, None, None, [], None, None


def build_session_data(timestamp):
    """
    ì„¸ì…˜ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ êµ¬ì„± (ìê¸°íš¨ëŠ¥ê° í•„ë“œ + Task Completion Check ì¶”ê°€)
    
    ===== CSV ë°ì´í„° êµ¬ì¡° ë¬¸ì„œí™” =====
    ì´ í•¨ìˆ˜ëŠ” ì‹¤í—˜ ì™„ë£Œ í›„ ì €ì¥ë˜ëŠ” ì£¼ìš” CSV íŒŒì¼ì˜ ëª¨ë“  ì»¬ëŸ¼ì„ ì •ì˜í•©ë‹ˆë‹¤.
    íŒŒì¼ëª… í˜•ì‹: korean_session{N}_{session_id}_{timestamp}.csv
    
    ğŸ“‹ ì£¼ìš” ë°ì´í„° ì¹´í…Œê³ ë¦¬:
    1. ê¸°ë³¸ ì‹ë³„ ì •ë³´ (session_id, timestamp ë“±)
    2. ë°°ê²½ ì •ë³´ ë° ì‚¬ì „ ì¸¡ì • (í•™ìŠµê¸°ê°„, ìì‹ ê°, ìê¸°íš¨ëŠ¥ê° 6ê°œ í•­ëª©)
    3. ì‹¤í—˜ í•µì‹¬ ë°ì´í„° (ì§ˆë¬¸, 1ì°¨/2ì°¨ ì „ì‚¬, ìŒì„± ê¸¸ì´)
    4. AI í”¼ë“œë°± ë¶„ì„ (JSON ì›ë³¸ + ì£¼ìš” í•„ë“œ ì¶”ì¶œ)
    5. ì—°êµ¬ìš© ì •ëŸ‰ ì ìˆ˜ (ì´ì¤‘ í‰ê°€ ì‹œìŠ¤í…œ: ì •í™•ì„±/ìœ ì°½ì„±)
    6. ê°œì„ ë„ í‰ê°€ (1ì°¨â†’2ì°¨ ë³€í™” ë¶„ì„)
    7. ë™ì˜ì„œ ë° ìœ¤ë¦¬ ì •ë³´ (GDPR ì¤€ìˆ˜)
    8. ë°ì´í„° í’ˆì§ˆ ë° ê´€ë¦¬ (ë³´ê´€ê¸°ê°„, í’ˆì§ˆ ë¼ë²¨)
    9. Task Completion Check (ë‘ ì£¼ì œ ì»¤ë²„ ì—¬ë¶€)
    
    ğŸ“Š í™œìš© ëª©ì :
    - í”¼ë“œë°± ì‹œìŠ¤í…œ íš¨ê³¼ì„± ë¶„ì„ (1ì°¨â†’2ì°¨ ê°œì„ ë„)
    - ìê¸°íš¨ëŠ¥ê°ê³¼ í•™ìŠµ ì„±ê³¼ ìƒê´€ê´€ê³„ ë¶„ì„  
    - ë°œí™” ê¸¸ì´ ìµœì í™” (ëª©í‘œ: 60-120ì´ˆ)
    - AI vs ì „ë¬¸ê°€ ì±„ì  ë¹„êµ ì—°êµ¬
    - í•™ìŠµì ìœ í˜•ë³„ ë§ì¶¤ í”¼ë“œë°± ê°œë°œ
    - Task Completion ë¶„ì„ (ì£¼ì œ ëˆ„ë½ íŒ¨í„´ ì—°êµ¬)
    
    Args:
        timestamp: íƒ€ì„ìŠ¤íƒ¬í”„
        
    Returns:
        dict: ì™„ì„±ëœ ì„¸ì…˜ ë°ì´í„° (CSV ì €ì¥ìš©)
    """
    research_scores = getattr(st.session_state, 'research_scores', {})
    
    # ê¸°ë³¸ê°’ ì„¤ì •
    default_research_scores = {
        'accuracy_score': 0.0,
        'fluency_score': 0.0,
        'error_rate': 0.0,
        'word_count': 0,
        'duration_s': 0.0,
        'error_count': 0
    }
    
    for key, default_value in default_research_scores.items():
        if key not in research_scores:
            research_scores[key] = default_value
    
    # Task Completion Check ë°ì´í„° ì¶”ì¶œ
    task_check_data = extract_task_completion_check(st.session_state.feedback.get('detailed_feedback', ''))

    session_data = {
        # ===== 1. ê¸°ë³¸ ì‹ë³„ ì •ë³´ =====
        'session_id': st.session_state.session_id,  # ì‹¤í—˜ ì„¸ì…˜ ê³ ìœ ë²ˆí˜¸ (ìµëª…í™”ëœ ID)
        'session_number': getattr(st.session_state, 'session_number', CURRENT_SESSION),  # ì„¸ì…˜ ì°¨ìˆ˜ (1 or 2)
        'session_label': getattr(st.session_state, 'session_label', SESSION_LABELS.get(CURRENT_SESSION, "Session 1")),  # ì„¸ì…˜ ë¼ë²¨
        'original_nickname': getattr(st.session_state, 'original_nickname', ''),  # ì°¸ì—¬ìê°€ ì…ë ¥í•œ ì›ë˜ ë‹‰ë„¤ì„ (ì—°êµ¬ì ì°¸ì¡°ìš©)
        'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),  # ì‹¤í—˜ ì™„ë£Œ ì‹œê°
        
        # ===== 2. ë°°ê²½ ì •ë³´ ë° ì‚¬ì „ ì¸¡ì • =====
        'learning_duration': getattr(st.session_state, 'learning_duration', ''),  # í•œêµ­ì–´ í•™ìŠµ ê¸°ê°„ ì„ íƒì§€
        'speaking_confidence': getattr(st.session_state, 'speaking_confidence', ''),  # ë§í•˜ê¸° ìì‹ ê° 5ì  ì²™ë„
        
        # ìê¸°íš¨ëŠ¥ê° 6ê°œ í•­ëª© (ê° 1-5ì  ë¦¬ì»¤íŠ¸ ì²™ë„)
        'self_efficacy_1': getattr(st.session_state, 'self_efficacy_1', ''),  # "I can use grammar accurately when speaking Korean"
        'self_efficacy_2': getattr(st.session_state, 'self_efficacy_2', ''),  # "I can use appropriate words when speaking Korean"
        'self_efficacy_3': getattr(st.session_state, 'self_efficacy_3', ''),  # "I can deliver what I want to say in Korean with confidence"
        'self_efficacy_4': getattr(st.session_state, 'self_efficacy_4', ''),  # "I can express my ideas clearly in Korean"
        'self_efficacy_5': getattr(st.session_state, 'self_efficacy_5', ''),  # "I can answer related questions well in Korean"
        'self_efficacy_6': getattr(st.session_state, 'self_efficacy_6', ''),  # "I can improve my speaking on my own through feedback"
        
        # ===== 3. ë™ì˜ì„œ ë° ìœ¤ë¦¬ ì •ë³´ =====
        'consent_given': getattr(st.session_state, 'consent_given', False),  # ë™ì˜ì„œ ì‘ì„± ì™„ë£Œ ì—¬ë¶€
        'consent_timestamp': getattr(st.session_state, 'consent_timestamp', ''),  # ë™ì˜ì„œ ì‘ì„± ì‹œê°
        'consent_participation': getattr(st.session_state, 'consent_participation', False),  # ì—°êµ¬ ì°¸ì—¬ ë™ì˜
        'consent_audio_ai': getattr(st.session_state, 'consent_audio_ai', False),  # ìŒì„± AI ë¶„ì„ ë™ì˜
        'consent_data_storage': getattr(st.session_state, 'consent_data_storage', False),  # ë°ì´í„° ì €ì¥ ë™ì˜
        'consent_privacy_rights': getattr(st.session_state, 'consent_privacy_rights', False),  # ê°œì¸ì •ë³´ ê¶Œë¦¬ ì¸ì§€
        'consent_final_confirmation': getattr(st.session_state, 'consent_final_confirmation', False),  # ìµœì¢… í™•ì¸
        'consent_zoom_interview': getattr(st.session_state, 'consent_zoom_interview', False),  # Zoom ì¸í„°ë·° ë™ì˜
        'gdpr_compliant': getattr(st.session_state, 'gdpr_compliant', False),  # GDPR ì¤€ìˆ˜ ì—¬ë¶€
        'consent_file_generated': getattr(st.session_state, 'consent_file', '') != '',  # ë™ì˜ì„œ íŒŒì¼ ìƒì„± ì—¬ë¶€
        'consent_file_filename': getattr(st.session_state, 'consent_file', ''),  # ë™ì˜ì„œ íŒŒì¼ëª…
        'consent_file_type': 'html',  # ë™ì˜ì„œ íŒŒì¼ í˜•ì‹ (HTML: í•œêµ­ì–´ ì§€ì›)
        
        # ===== 4. ì‹¤í—˜ í•µì‹¬ ë°ì´í„° =====
        'question': EXPERIMENT_QUESTION,  # ì‹¤í—˜ì—ì„œ ì œì‹œëœ ì§ˆë¬¸ (í•œêµ­ì–´)
        'transcription_1': st.session_state.transcription_1,  # ì²« ë²ˆì§¸ ë…¹ìŒ STT ì „ì‚¬ ê²°ê³¼
        'transcription_2': st.session_state.transcription_2,  # ë‘ ë²ˆì§¸ ë…¹ìŒ STT ì „ì‚¬ ê²°ê³¼ (í”¼ë“œë°± ì ìš© í›„)
        'gpt_feedback_json': json.dumps(st.session_state.feedback, ensure_ascii=False),  # GPTê°€ ìƒì„±í•œ ì „ì²´ í”¼ë“œë°± (JSON ì›ë³¸)
        
        # ===== 4.5. Task Completion Check ë°ì´í„° (ìƒˆë¡œ ì¶”ê°€) =====
        'task_check_past_vacation': task_check_data.get('past_vacation_status', 'Unknown'),  # Covered/Partially/Missing
        'task_check_future_plans': task_check_data.get('future_plans_status', 'Unknown'),  # Covered/Partially/Missing
        'task_check_tense_usage': task_check_data.get('tense_usage', 'Unknown'),  # ì‹œì œ ì‚¬ìš© í‰ê°€
        'task_check_both_topics_covered': task_check_data.get('both_topics_covered', False),  # ë‘ ì£¼ì œ ëª¨ë‘ ë‹¤ë¤˜ëŠ”ì§€
        'task_check_json': json.dumps(task_check_data, ensure_ascii=False),  # ì „ì²´ Task Check ë°ì´í„°
        
        # ===== 5. ì—°êµ¬ìš© ì •ëŸ‰ ì ìˆ˜ (ì´ì¤‘ í‰ê°€ ì‹œìŠ¤í…œ) =====
        # ë…¼ë¬¸ìš© ê°ê´€ì  ì ìˆ˜ (ì˜¤ë¥˜ìœ¨, ë‹¨ì–´ìˆ˜ ê¸°ë°˜ ìë™ ê³„ì‚°)
        'research_accuracy_score': research_scores.get('accuracy_score', 0.0),  # ì •í™•ì„± ì ìˆ˜ (0-10ì , ì˜¤ë¥˜ìœ¨ ê¸°ë°˜)
        'research_fluency_score': research_scores.get('fluency_score', 0.0),  # ìœ ì°½ì„± ì ìˆ˜ (0-10ì , ë‹¨ì–´ìˆ˜ ê¸°ë°˜)
        'research_error_rate': research_scores.get('error_rate', 0.0),  # ì˜¤ë¥˜ìœ¨ (%, ë‹¨ì–´ë‹¹ ë¬¸ë²• ì˜¤ë¥˜)
        'research_word_count': research_scores.get('word_count', 0),  # ì´ ë‹¨ì–´ ìˆ˜
        'research_duration_s': research_scores.get('duration_s', 0.0),  # ë…¹ìŒ ê¸¸ì´ (ì´ˆ)
        'research_error_count': research_scores.get('error_count', 0),  # ì‹¤ì œ ë¬¸ë²• ì˜¤ë¥˜ ê°œìˆ˜
        'research_scores_json': json.dumps(research_scores, ensure_ascii=False),  # ì—°êµ¬ìš© ì ìˆ˜ ì „ì²´ (JSON)
        
        # ===== 6. ë°ì´í„° í’ˆì§ˆ ë¶„ì„ =====
        'audio_duration_1': getattr(st.session_state, 'audio_duration_1', 0.0),  # ì²« ë²ˆì§¸ ë…¹ìŒ ê¸¸ì´ (ì´ˆ)
        'audio_duration_2': getattr(st.session_state, 'audio_duration_2', 0.0),  # ë‘ ë²ˆì§¸ ë…¹ìŒ ê¸¸ì´ (ì´ˆ)
        'audio_quality_check_1': get_audio_quality_label(getattr(st.session_state, 'audio_duration_1', 0)),  # ì²« ë²ˆì§¸ ë…¹ìŒ í’ˆì§ˆ ë¼ë²¨
        'audio_quality_check_2': get_audio_quality_label(getattr(st.session_state, 'audio_duration_2', 0)),  # ë‘ ë²ˆì§¸ ë…¹ìŒ í’ˆì§ˆ ë¼ë²¨
        
        # ===== 7. ê°œì„ ë„ í‰ê°€ ë°ì´í„° =====
        'improvement_score': getattr(st.session_state, 'improvement_assessment', {}).get('improvement_score', 0),  # 1ì°¨â†’2ì°¨ ê°œì„ ë„ ì ìˆ˜ (1-10ì )
        'improvement_reason': getattr(st.session_state, 'improvement_assessment', {}).get('improvement_reason', ''),  # ê°œì„ ë„ í‰ê°€ ì´ìœ 
        'first_attempt_score': getattr(st.session_state, 'improvement_assessment', {}).get('first_attempt_score', 0),  # 1ì°¨ ì‹œë„ í‰ê°€ ì ìˆ˜
        'second_attempt_score': getattr(st.session_state, 'improvement_assessment', {}).get('second_attempt_score', 0),  # 2ì°¨ ì‹œë„ í‰ê°€ ì ìˆ˜
        'score_difference': getattr(st.session_state, 'improvement_assessment', {}).get('score_difference', 0),  # ì ìˆ˜ ì°¨ì´ (2ì°¨ - 1ì°¨)
        'feedback_application': getattr(st.session_state, 'improvement_assessment', {}).get('feedback_application', ''),  # í”¼ë“œë°± ì ìš©ë„ ("excellent/good/partial/minimal")
        'specific_improvements': json.dumps(getattr(st.session_state, 'improvement_assessment', {}).get('specific_improvements', []), ensure_ascii=False),  # êµ¬ì²´ì  ê°œì„ ì‚¬í•­ ëª©ë¡
        'remaining_issues': json.dumps(getattr(st.session_state, 'improvement_assessment', {}).get('remaining_issues', []), ensure_ascii=False),  # ë‚¨ì€ ê³¼ì œ ëª©ë¡
        'overall_assessment': getattr(st.session_state, 'improvement_assessment', {}).get('overall_assessment', ''),  # ì „ì²´ í‰ê°€ ìš”ì•½
        'improvement_assessment_json': json.dumps(getattr(st.session_state, 'improvement_assessment', {}), ensure_ascii=False),  # ê°œì„ ë„ í‰ê°€ ì „ì²´ (JSON)
        
        # ===== 8. í•™ìƒìš© í”¼ë“œë°± í•„ë“œë“¤ (UI í‘œì‹œìš©) =====
        'suggested_model_sentence': st.session_state.feedback.get('suggested_model_sentence', ''),  # AIê°€ ì œì•ˆí•œ ëª¨ë²” ë‹µì•ˆ (í•œêµ­ì–´)
        'suggested_model_sentence_english': st.session_state.feedback.get('suggested_model_sentence_english', ''),  # ëª¨ë²” ë‹µì•ˆ ì˜ì–´ ë²ˆì—­
        'fluency_comment': st.session_state.feedback.get('fluency_comment', ''),  # ìœ ì°½ì„± ì½”ë©˜íŠ¸
        'interview_readiness_score': st.session_state.feedback.get('interview_readiness_score', ''),  # AIê°€ í‰ê°€í•œ ì¸í„°ë·° ì¤€ë¹„ë„ (1-10ì )
        'interview_readiness_reason': st.session_state.feedback.get('interview_readiness_reason', ''),  # ì¸í„°ë·° ì¤€ë¹„ë„ ì ìˆ˜ ì´ìœ 
        'grammar_expression_tip': st.session_state.feedback.get('grammar_expression_tip', ''),  # ê³ ê¸‰ ë¬¸ë²• íŒ¨í„´ ì œì•ˆ
        
        # ===== 9. STT ë£¨ë¸Œë¦­ ê¸°ë°˜ í”¼ë“œë°± ë¶„ì„ =====
        'grammar_issues_count': len(st.session_state.feedback.get('grammar_issues', [])),  # ë°œê²¬ëœ ë¬¸ë²• ì˜¤ë¥˜ ê°œìˆ˜
        'vocabulary_suggestions_count': len(st.session_state.feedback.get('vocabulary_suggestions', [])),  # ì–´íœ˜ ì œì•ˆ ê°œìˆ˜
        'content_expansion_suggestions_count': len(st.session_state.feedback.get('content_expansion_suggestions', [])),  # ë‚´ìš© í™•ì¥ ì œì•ˆ ê°œìˆ˜
        'content_expansion_suggestions_json': json.dumps(st.session_state.feedback.get('content_expansion_suggestions', []), ensure_ascii=False),  # ë‚´ìš© í™•ì¥ ì œì•ˆ ìƒì„¸
        'grammar_issues_json': json.dumps(st.session_state.feedback.get('grammar_issues', []), ensure_ascii=False),  # ë¬¸ë²• ì˜¤ë¥˜ ìƒì„¸ ë¶„ì„
        'vocabulary_suggestions_json': json.dumps(st.session_state.feedback.get('vocabulary_suggestions', []), ensure_ascii=False),  # ì–´íœ˜ ì œì•ˆ ìƒì„¸
        'highlight_targets_json': json.dumps(st.session_state.feedback.get('highlight_targets', {}), ensure_ascii=False),  # í•˜ì´ë¼ì´íŠ¸ ëŒ€ìƒ ë¶„ì„
        
        # ===== 10. ë””ë²„ê·¸ ë° ì‹œìŠ¤í…œ ì •ë³´ =====
        'gpt_model_used': st.session_state.gpt_debug_info.get('model_used', ''),  # ì‚¬ìš©ëœ GPT ëª¨ë¸ëª…
        'gpt_attempts': st.session_state.gpt_debug_info.get('attempts', 0),  # GPT API ì‹œë„ íšŸìˆ˜
        'dual_evaluation_used': st.session_state.gpt_debug_info.get('dual_evaluation', False),  # ì´ì¤‘ í‰ê°€ ì‹œìŠ¤í…œ ì‚¬ìš© ì—¬ë¶€
        
        # ===== 11. íŒŒì¼ ê´€ë¦¬ ì •ë³´ =====
        'audio_folder': f"{FOLDERS['audio_recordings']}/{getattr(st.session_state, 'session_number', CURRENT_SESSION)}_{st.session_state.session_id}_{timestamp}",  # ìŒì„± íŒŒì¼ ì €ì¥ í´ë”
        'data_retention_until': (datetime.now() + timedelta(days=DATA_RETENTION_DAYS)).strftime('%Y-%m-%d'),  # ë°ì´í„° ë³´ê´€ ë§Œë£Œì¼
        'deletion_requested': False,  # ì‚­ì œ ìš”ì²­ ì—¬ë¶€
        'last_updated': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),  # ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°
        'saved_at_step': 'second_recording_complete',  # ì €ì¥ ì‹œì  ë‹¨ê³„
        'save_trigger': 'auto_after_second_recording'  # ì €ì¥ íŠ¸ë¦¬ê±° ë°©ì‹
    }
    
    return session_data

def get_audio_quality_label(duration):
    """
    ìŒì„± ê¸¸ì´ì— ë”°ë¥¸ í’ˆì§ˆ ë¼ë²¨ ë°˜í™˜ (1-2ë¶„ ëª©í‘œ ê¸°ì¤€)
    
    Args:
        duration: ìŒì„± ê¸¸ì´ (ì´ˆ)
        
    Returns:
        str: í’ˆì§ˆ ë¼ë²¨
    """
    if duration >= 90:  # 1.5ë¶„ (ì¤‘ê°„ê°’)
        return 'excellent'
    elif duration >= 75:  # 1ë¶„ 15ì´ˆ
        return 'good'
    elif duration >= 60:  # 1ë¶„
        return 'fair'
    else:
        return 'very_short'


def calculate_self_efficacy_average():
    """
    ìê¸°íš¨ëŠ¥ê° í‰ê·  ê³„ì‚°
    
    Returns:
        float: ìê¸°íš¨ëŠ¥ê° í‰ê·  (1-5ì )
    """
    efficacy_scores = []
    for i in range(1, 7):
        score = getattr(st.session_state, f'self_efficacy_{i}', 0)
        if score and isinstance(score, (int, float)) and 1 <= score <= 5:
            efficacy_scores.append(score)
    
    return round(sum(efficacy_scores) / len(efficacy_scores), 2) if efficacy_scores else 0


def save_to_csv(session_data, timestamp):
    """
    ì„¸ì…˜ ë°ì´í„°ë¥¼ CSVë¡œ ì €ì¥
    
    Args:
        session_data: ì„¸ì…˜ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        timestamp: íƒ€ì„ìŠ¤íƒ¬í”„
        
    Returns:
        str: CSV íŒŒì¼ ê²½ë¡œ
    """
    session_num = getattr(st.session_state, 'session_number', CURRENT_SESSION)
    csv_filename = os.path.join(
        FOLDERS["data"], 
        f"korean_session{session_num}_{st.session_state.session_id}_{timestamp}.csv"
    )
    
    with open(csv_filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=session_data.keys())
        writer.writeheader()
        writer.writerow(session_data)
    
    return csv_filename


def save_audio_files(timestamp):
    """
    ìŒì„± íŒŒì¼ë“¤ì„ ì €ì¥
    
    Args:
        timestamp: íƒ€ì„ìŠ¤íƒ¬í”„
        
    Returns:
        tuple: (folder_path, saved_files_list)
    """
    try:
        session_num = getattr(st.session_state, 'session_number', CURRENT_SESSION)
        folder_name = os.path.join(
            FOLDERS["audio_recordings"], 
            f"session{session_num}_{st.session_state.session_id}_{timestamp}"
        )
        os.makedirs(folder_name, exist_ok=True)
        
        saved_files = []
        
        # ì²« ë²ˆì§¸ ë…¹ìŒ
        if hasattr(st.session_state, 'first_audio') and st.session_state.first_audio:
            file_path = os.path.join(folder_name, "first_audio.wav")
            with open(file_path, "wb") as f:
                f.write(st.session_state.first_audio["bytes"])
            saved_files.append("first_audio.wav")
        
        # ë‘ ë²ˆì§¸ ë…¹ìŒ
        if hasattr(st.session_state, 'second_audio') and st.session_state.second_audio:
            file_path = os.path.join(folder_name, "second_audio.wav")
            with open(file_path, "wb") as f:
                f.write(st.session_state.second_audio["bytes"])
            saved_files.append("second_audio.wav")
        
        # ëª¨ë¸ ìŒì„± (ì¼ë°˜ ì†ë„)
        if st.session_state.model_audio.get("normal"):
            file_path = os.path.join(folder_name, "model_normal.mp3")
            with open(file_path, "wb") as f:
                f.write(st.session_state.model_audio["normal"])
            saved_files.append("model_normal.mp3")
        
        # ëª¨ë¸ ìŒì„± (ëŠë¦° ì†ë„)
        if st.session_state.model_audio.get("slow"):
            file_path = os.path.join(folder_name, "model_slow.mp3")
            with open(file_path, "wb") as f:
                f.write(st.session_state.model_audio["slow"])
            saved_files.append("model_slow.mp3")
        
        return folder_name, saved_files
    
    except Exception as e:
        st.error(f"âŒ Error saving audio files: {str(e)}")
        return None, []


def create_participant_info_file(session_id, timestamp):
    """
    ì°¸ì—¬ì ì •ë³´ íŒŒì¼ ìƒì„± (ìê¸°íš¨ëŠ¥ê° + Task Completion Check í¬í•¨)
    
    Args:
        session_id: ì„¸ì…˜ ID
        timestamp: íƒ€ì„ìŠ¤íƒ¬í”„
        
    Returns:
        str: ìƒì„±ëœ íŒŒì¼ ê²½ë¡œ
    """
    try:
        info_filename = os.path.join(FOLDERS["data"], f"{session_id}_participant_info.txt")
        
        original_nickname = getattr(st.session_state, 'original_nickname', 'Unknown')
        session_label = getattr(st.session_state, 'session_label', SESSION_LABELS.get(CURRENT_SESSION, "Session 1"))
        learning_duration = getattr(st.session_state, 'learning_duration', 'Not specified')
        speaking_confidence = getattr(st.session_state, 'speaking_confidence', 'Not specified')
        
        # ìê¸°íš¨ëŠ¥ê° ì ìˆ˜ ìˆ˜ì§‘ (6ê°œ)
        efficacy_scores = []
        for i in range(1, 7):
            score = getattr(st.session_state, f'self_efficacy_{i}', 'N/A')
            efficacy_scores.append(f"Item {i}: {score}/5")
        
        efficacy_avg = calculate_self_efficacy_average()
        
        research_scores = getattr(st.session_state, 'research_scores', {})
        accuracy_score = research_scores.get('accuracy_score', 'N/A')
        fluency_score = research_scores.get('fluency_score', 'N/A')
        error_rate = research_scores.get('error_rate', 'N/A')
        word_count = research_scores.get('word_count', 'N/A')
        
        # Task Completion Check ì •ë³´ ì¶”ì¶œ
        task_check_data = extract_task_completion_check(st.session_state.feedback.get('detailed_feedback', ''))
        
        info_content = f"""=== PARTICIPANT INFORMATION ===
Anonymous ID: {session_id}
Original Nickname: {original_nickname}
Session: {session_label} (Session {CURRENT_SESSION})
Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Save Trigger: Auto-save after second recording completion

=== BACKGROUND INFORMATION ===
Learning Duration: {learning_duration}
Speaking Confidence: {speaking_confidence}

=== SELF-EFFICACY SCORES (1-5 scale) ===
{chr(10).join(efficacy_scores)}
Average Self-Efficacy: {efficacy_avg}/5.0

=== TASK COMPLETION CHECK ===
Past Vacation Coverage: {task_check_data.get('past_vacation_status', 'Unknown')}
Future Plans Coverage: {task_check_data.get('future_plans_status', 'Unknown')}
Tense Usage: {task_check_data.get('tense_usage', 'Unknown')}
Both Topics Covered: {'Yes' if task_check_data.get('both_topics_covered') else 'No'}
Missing Topics: {', '.join(task_check_data.get('missing_topics', [])) if task_check_data.get('missing_topics') else 'None'}

=== EXPERIMENT DETAILS ===
Question: {EXPERIMENT_QUESTION}
First Recording Duration: {getattr(st.session_state, 'audio_duration_1', 0):.1f} seconds
Second Recording Duration: {getattr(st.session_state, 'audio_duration_2', 0):.1f} seconds
Student UI Score: {st.session_state.feedback.get('interview_readiness_score', 'N/A')}/10

=== RESEARCH SCORES ===
Accuracy Score: {accuracy_score}/10 (Error rate: {error_rate}%)
Fluency Score: {fluency_score}/10 (Word count: {word_count})
Dual Evaluation System: {getattr(st.session_state, 'gpt_debug_info', {}).get('dual_evaluation', False)}

=== CONSENT INFORMATION ===
Consent Given: {getattr(st.session_state, 'consent_given', False)}
Consent Timestamp: {getattr(st.session_state, 'consent_timestamp', 'N/A')}
GDPR Compliant: {getattr(st.session_state, 'gdpr_compliant', False)}
Zoom Interview Consent: {getattr(st.session_state, 'consent_zoom_interview', False)}
Consent File Type: HTML (Korean language support)

=== DATA MANAGEMENT ===
Data Retention Until: {(datetime.now() + timedelta(days=DATA_RETENTION_DAYS)).strftime('%Y-%m-%d')}
Storage Method: GCS ZIP Archive (Auto-save after 2nd recording)
Last Updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

=== FOR RESEARCHER ===
This file contains the link between the anonymous ID and the original nickname.
Data was automatically saved after second recording completion.
Self-efficacy scores (1-5 scale) collected before experiment.
Task Completion Check shows whether both topics (past/future) were covered.
Consent form is stored as HTML file for Korean language compatibility.
TOPIK reference scores (3 areas) stored in Excel file: reference_scores_{timestamp}.xlsx

Contact: pen0226@gmail.com for any data requests or questions.
"""
        
        with open(info_filename, 'w', encoding='utf-8') as f:
            f.write(info_content)
        
        return info_filename
    
    except Exception as e:
        return None
    
    

def create_comprehensive_backup_zip(session_id, timestamp, reference_excel_filename=None):
    """
    ëª¨ë“  ì„¸ì…˜ ë°ì´í„°ë¥¼ í¬í•¨í•œ ì™„ì „í•œ ë°±ì—… ZIP ìƒì„± (TOPIK ì—‘ì…€ í¬í•¨ ë³´ì¥)
    
    Args:
        session_id: ì„¸ì…˜ ID
        timestamp: íƒ€ì„ìŠ¤íƒ¬í”„
        reference_excel_filename: ì°¸ê³ ìš© ì—‘ì…€ íŒŒì¼ ê²½ë¡œ (ëª…ì‹œì  ì „ë‹¬)
        
    Returns:
        str: ZIP íŒŒì¼ ê²½ë¡œ ë˜ëŠ” None
    """
    try:
        session_num = getattr(st.session_state, 'session_number', CURRENT_SESSION)
        zip_filename = os.path.join(
            FOLDERS["data"], 
            f"{session_id}_{timestamp}.zip"
        )
        
        with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
            
            # ì°¸ì—¬ì ì •ë³´ íŒŒì¼ ìƒì„± ë° ì¶”ê°€
            participant_info_file = create_participant_info_file(session_id, timestamp)
            if participant_info_file and os.path.exists(participant_info_file):
                zipf.write(participant_info_file, "participant_info.txt")
            
            # CSV íŒŒì¼ ì¶”ê°€
            csv_file = os.path.join(FOLDERS["data"], f"korean_session{session_num}_{session_id}_{timestamp}.csv")
            if os.path.exists(csv_file):
                zipf.write(csv_file, f"session_data_{timestamp}.csv")
                print(f"âœ… CSV file included: session_data_{timestamp}.csv")
            
            # ğŸ”¥ TOPIK ì°¸ê³ ìš© ì—‘ì…€ íŒŒì¼ ì¶”ê°€ (í™•ì‹¤í•œ ê²½ë¡œ í™•ì¸)
            excel_included = False
            
            # ë°©ë²• 1: ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬ë°›ì€ ê²½ë¡œ ì‚¬ìš©
            if reference_excel_filename and os.path.exists(reference_excel_filename):
                zipf.write(reference_excel_filename, f"reference_scores_{timestamp}.xlsx")
                print(f"âœ… Reference Excel file included (method 1): reference_scores_{timestamp}.xlsx")
                excel_included = True
            
            # ë°©ë²• 2: ì§ì ‘ ê²½ë¡œ êµ¬ì„±í•´ì„œ ì°¾ê¸°
            if not excel_included:
                reference_excel_direct = os.path.join(FOLDERS["data"], f"reference_scores_{timestamp}.xlsx")
                if os.path.exists(reference_excel_direct):
                    zipf.write(reference_excel_direct, f"reference_scores_{timestamp}.xlsx")
                    print(f"âœ… Reference Excel file included (method 2): reference_scores_{timestamp}.xlsx")
                    excel_included = True
            
            # ë°©ë²• 3: íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì°¾ê¸°
            if not excel_included:
                import glob
                pattern = os.path.join(FOLDERS["data"], f"reference_scores_*.xlsx")
                excel_files = glob.glob(pattern)
                if excel_files:
                    # ê°€ì¥ ìµœì‹  íŒŒì¼ ë˜ëŠ” timestamp ë§¤ì¹­í•˜ëŠ” íŒŒì¼ ì‚¬ìš©
                    matching_files = [f for f in excel_files if timestamp in f]
                    if matching_files:
                        latest_excel = matching_files[0]
                    else:
                        latest_excel = max(excel_files, key=os.path.getctime)
                    
                    zipf.write(latest_excel, f"reference_scores_{timestamp}.xlsx")
                    print(f"âœ… Reference Excel file included (method 3): reference_scores_{timestamp}.xlsx")
                    excel_included = True
            
            if not excel_included:
                print(f"âš ï¸ Reference Excel file NOT FOUND for inclusion in ZIP")
                print(f"   Checked paths:")
                print(f"   - {reference_excel_filename}")
                print(f"   - {os.path.join(FOLDERS['data'], f'reference_scores_{timestamp}.xlsx')}")
            
            # HTML ë™ì˜ì„œ íŒŒì¼ ì¶”ê°€
            consent_html = os.path.join(FOLDERS["data"], f"{session_id}_consent.html")
            if os.path.exists(consent_html):
                zipf.write(consent_html, f"consent_form_{session_id}.html")
                print(f"âœ… Consent HTML file included: {session_id}_consent.html")
            else:
                print(f"âš ï¸ Consent HTML file not found: {session_id}_consent.html")
            
            # ìŒì„± íŒŒì¼ë“¤ ì¶”ê°€
            audio_folder = os.path.join(FOLDERS["audio_recordings"], f"session{session_num}_{session_id}_{timestamp}")
            if os.path.exists(audio_folder):
                for file in os.listdir(audio_folder):
                    file_path = os.path.join(audio_folder, file)
                    zipf.write(file_path, f"audio/{file}")
                print(f"âœ… Audio files included from: {audio_folder}")
            
            # ZIP ë‚´ìš© ìš”ì•½ íŒŒì¼ ì¶”ê°€
            research_scores = getattr(st.session_state, 'research_scores', {})
            efficacy_avg = calculate_self_efficacy_average()
            
            readme_content = f"""=== ZIP CONTENTS SUMMARY ===
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Participant: {session_id} (Session {session_num})
Save Trigger: Auto-save after second recording completion

Files included:
- participant_info.txt: Participant details + Research scores + Self-efficacy scores
- session_data_{timestamp}.csv: Complete session data with dual evaluation + self-efficacy data
- reference_scores_{timestamp}.xlsx: TOPIK holistic reference scores for both attempts {'âœ… INCLUDED' if excel_included else 'âŒ MISSING'}
- consent_form_{session_id}.html: Signed consent form (HTML format for Korean support)
- audio/: All recorded audio files (student + model pronunciations)

SELF-EFFICACY DATA:
- 6 items measured on 1-5 scale
- Average self-efficacy score: {efficacy_avg}/5.0
- Individual scores stored in CSV under self_efficacy_1 through self_efficacy_6

TOPIK REFERENCE SCORES:
- Holistic rubric scoring: Content/Task, Language Use, Delivery (STT-based)
- Each area scored 1-5 points (holistic impression-based)
- Total score: simple sum (3-15 points)

CONSENT FORM FORMAT:
- HTML format for perfect Korean language support
- Can be saved as PDF using browser print function (Ctrl+P)
- Avoids character encoding issues that occurred with direct PDF generation

RESEARCH WORKFLOW:
1. Use CSV data for basic analysis
2. Use Excel file for TOPIK reference scores (3 areas)
3. Raw audio files available for manual grading
4. All raw data preserved for transparency

IMPORTANT: Data was automatically saved after second recording completion.
This ensures no data loss even if survey is not completed.

Contact researcher: pen0226@gmail.com
"""
            
            readme_path = os.path.join(FOLDERS["data"], f"{session_id}_readme.txt")
            with open(readme_path, 'w', encoding='utf-8') as f:
                f.write(readme_content)
            zipf.write(readme_path, "README.txt")
        
        # ì„ì‹œ íŒŒì¼ë“¤ ì •ë¦¬
        temp_files = [participant_info_file, readme_path]
        for temp_file in temp_files:
            if temp_file and os.path.exists(temp_file):
                try:
                    os.remove(temp_file)
                except:
                    pass
        
        print(f"âœ… Comprehensive backup ZIP created: {zip_filename}")
        return zip_filename
        
    except Exception as e:
        st.error(f"âŒ Error creating comprehensive backup ZIP: {e}")
        return None


def get_gcs_client():
    """
    GCS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    
    Returns:
        tuple: (client, bucket, status_message)
    """
    try:
        from google.cloud import storage
        import json
        
        if not GCS_ENABLED:
            return None, None, "GCS upload is disabled in configuration"
        
        if not GCS_SERVICE_ACCOUNT:
            return None, None, "GCS service account not configured"
        
        try:
            if isinstance(GCS_SERVICE_ACCOUNT, dict):
                credentials_dict = dict(GCS_SERVICE_ACCOUNT)
                print(f"Using TOML format service account (Project: {credentials_dict.get('project_id', 'Unknown')})")
            elif isinstance(GCS_SERVICE_ACCOUNT, str):
                credentials_dict = json.loads(GCS_SERVICE_ACCOUNT)
                print(f"Using JSON format service account (Project: {credentials_dict.get('project_id', 'Unknown')})")
            else:
                return None, None, f"Unexpected service account type: {type(GCS_SERVICE_ACCOUNT)}"
                
        except json.JSONDecodeError:
            return None, None, "Invalid JSON format in service account"
        except Exception as parse_error:
            return None, None, f"Service account parsing error: {str(parse_error)}"
        
        client = storage.Client.from_service_account_info(credentials_dict)
        bucket = client.bucket(GCS_BUCKET_NAME)
        
        return client, bucket, "Success"
        
    except ImportError as e:
        return None, None, f"Missing required libraries: {str(e)}. Run: pip install google-cloud-storage"
    except Exception as e:
        return None, None, f"GCS client initialization failed: {str(e)}"


def upload_to_gcs(local_path, blob_name):
    """
    GCSì— íŒŒì¼ ì—…ë¡œë“œ
    
    Args:
        local_path: ì—…ë¡œë“œí•  ë¡œì»¬ íŒŒì¼ ê²½ë¡œ
        blob_name: GCSì—ì„œ ì‚¬ìš©í•  íŒŒì¼ëª…
        
    Returns:
        tuple: (blob_url, status_message)
    """
    try:
        client, bucket, status = get_gcs_client()
        if not client:
            return None, f"GCS client error: {status}"
        
        if not os.path.exists(local_path):
            return None, f"File not found: {local_path}"
        
        blob = bucket.blob(blob_name)
        blob.upload_from_filename(local_path)
        
        blob_url = f"gs://{GCS_BUCKET_NAME}/{blob_name}"
        print(f"File uploaded: {blob_name}")
        
        return blob_url, f"Successfully uploaded: {blob_name}"
        
    except Exception as e:
        return None, f"Upload failed: {str(e)}"


def check_mapping_file_freshness():
    """
    ë§¤í•‘ íŒŒì¼ì˜ ìµœì‹  ìƒíƒœ í™•ì¸ (consent.pyì—ì„œ ì‹¤ì‹œê°„ ì—…ë¡œë“œí–ˆëŠ”ì§€ ì²´í¬)
    
    Returns:
        tuple: (needs_upload, reason)
    """
    try:
        # í˜„ì¬ ì„¸ì…˜ì˜ ë‹‰ë„¤ì„ìœ¼ë¡œ ìµœê·¼ ì—…ë°ì´íŠ¸ê°€ ìˆì—ˆëŠ”ì§€ í™•ì¸
        original_nickname = getattr(st.session_state, 'original_nickname', '')
        if not original_nickname:
            return False, "No nickname in current session"
        
        mapping_file = os.path.join(FOLDERS["data"], 'nickname_mapping.csv')
        if not os.path.exists(mapping_file):
            return False, "No local mapping file found"
        
        # í˜„ì¬ ì„¸ì…˜ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ í™•ì¸
        current_timestamp = datetime.now()
        
        with open(mapping_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                if row.get('Nickname', '').strip().lower() == original_nickname.lower():
                    # ë§¤í•‘ íŒŒì¼ì˜ ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ í™•ì¸
                    file_timestamp_str = row.get('Timestamp', '')
                    if file_timestamp_str:
                        try:
                            file_timestamp = datetime.strptime(file_timestamp_str, '%Y-%m-%d %H:%M:%S')
                            time_diff = (current_timestamp - file_timestamp).total_seconds()
                            
                            # 5ë¶„ ì´ë‚´ì— ì—…ë°ì´íŠ¸ë˜ì—ˆë‹¤ë©´ ìµœì‹  ìƒíƒœë¡œ ê°„ì£¼
                            if time_diff <= 300:  # 5ë¶„ = 300ì´ˆ
                                return False, f"Recently updated {time_diff:.0f}s ago by consent.py"
                            else:
                                return True, f"Last updated {time_diff:.0f}s ago, needs refresh"
                        except ValueError:
                            return True, "Invalid timestamp format, needs refresh"
                    break
        
        return True, "Nickname not found in mapping file, needs upload"
        
    except Exception as e:
        return True, f"Error checking freshness: {str(e)}"


def auto_backup_to_gcs(csv_filename, excel_filename, zip_filename, session_id, timestamp):
    """
    ZIP íŒŒì¼ë§Œ GCSì— ìë™ ë°±ì—… + ìŠ¤ë§ˆíŠ¸í•œ nickname_mapping.csv ë°±ì—…
    (consent.pyì—ì„œ ì‹¤ì‹œê°„ ì—…ë¡œë“œí–ˆë‹¤ë©´ ì¤‘ë³µ ì—…ë¡œë“œ ë°©ì§€)
    
    Args:
        csv_filename: CSV íŒŒì¼ ê²½ë¡œ
        excel_filename: ì°¸ê³ ìš© ì—‘ì…€ íŒŒì¼ ê²½ë¡œ (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ - ZIPì— í¬í•¨ë¨)
        zip_filename: ZIP íŒŒì¼ ê²½ë¡œ
        session_id: ì„¸ì…˜ ID
        timestamp: íƒ€ì„ìŠ¤íƒ¬í”„
        
    Returns:
        tuple: (uploaded_files, errors)
    """
    uploaded_files = []
    errors = []
    
    if not GCS_ENABLED:
        errors.append("GCS upload is disabled in configuration")
        return uploaded_files, errors
    
    session_num = getattr(st.session_state, 'session_number', CURRENT_SESSION)
    session_folder = GCS_SIMPLE_STRUCTURE.get(session_num, GCS_SIMPLE_STRUCTURE[1])
    
    # ZIP íŒŒì¼ë§Œ ì—…ë¡œë“œ (ì—‘ì…€ì€ ZIP ì•ˆì— í¬í•¨ë¨)
    if zip_filename and os.path.exists(zip_filename):
        try:
            blob_name = f"{session_folder}{session_id}_{timestamp}.zip"
            blob_url, result_msg = upload_to_gcs(zip_filename, blob_name)
            
            if blob_url:
                uploaded_files.append(blob_name)
                print(f"âœ… Session {session_num} ZIP uploaded: {blob_name}")
            else:
                errors.append(f"ZIP upload failed: {result_msg}")
                
        except Exception as e:
            errors.append(f"ZIP upload error: {str(e)}")
    else:
        errors.append("ZIP file not found for upload")
    
    # ğŸ”¥ ìŠ¤ë§ˆíŠ¸í•œ nickname_mapping.csv ë°±ì—… (ì¤‘ë³µ ë°©ì§€)
    needs_upload, reason = check_mapping_file_freshness()
    
    if needs_upload:
        mapping_file = os.path.join(FOLDERS["data"], 'nickname_mapping.csv')
        if os.path.exists(mapping_file):
            try:
                mapping_blob_name = "nickname_mapping.csv"
                blob_url, result_msg = upload_to_gcs(mapping_file, mapping_blob_name)
                
                if blob_url:
                    uploaded_files.append(mapping_blob_name)
                    print(f"ğŸ“ Mapping file uploaded: {reason}")
                else:
                    errors.append(f"Mapping file upload failed: {result_msg}")
                    
            except Exception as e:
                errors.append(f"Mapping file upload error: {str(e)}")
        else:
            print(f"ğŸ“ No local mapping file to upload")
    else:
        print(f"ğŸ“ Mapping file upload skipped: {reason}")
    
    return uploaded_files, errors


def test_gcs_connection():
    """
    GCS ì—°ê²° ìƒíƒœ í…ŒìŠ¤íŠ¸
    
    Returns:
        tuple: (success, message)
    """
    try:
        client, bucket, status = get_gcs_client()
        if not client:
            return False, f"GCS connection failed: {status}"
        
        if bucket.exists():
            project_id = "Unknown"
            if isinstance(GCS_SERVICE_ACCOUNT, dict):
                project_id = GCS_SERVICE_ACCOUNT.get('project_id', 'Unknown')
                format_type = "TOML format"
            else:
                import json
                try:
                    service_info = json.loads(GCS_SERVICE_ACCOUNT)
                    project_id = service_info.get('project_id', 'Unknown')
                    format_type = "JSON format"
                except:
                    format_type = "Unknown format"
            
            return True, f"Connected successfully to bucket: {GCS_BUCKET_NAME} (Project: {project_id} - {format_type})"
        else:
            return False, f"Bucket not found: {GCS_BUCKET_NAME}"
        
    except Exception as e:
        return False, f"Connection test failed: {str(e)}"


def log_upload_status(session_id, timestamp, uploaded_files, errors, email_sent=False):
    """
    GCS ì—…ë¡œë“œ ê²°ê³¼ë¥¼ ë¡œê·¸ íŒŒì¼ì— ê¸°ë¡ (ë§¤í•‘ íŒŒì¼ ë™ê¸°í™” ì •ë³´ í¬í•¨)
    
    Args:
        session_id: ì„¸ì…˜ ID
        timestamp: íƒ€ì„ìŠ¤íƒ¬í”„
        uploaded_files: ì—…ë¡œë“œëœ íŒŒì¼ ëª©ë¡
        errors: ì˜¤ë¥˜ ëª©ë¡
        email_sent: ì´ë©”ì¼ ì „ì†¡ ì—¬ë¶€
        
    Returns:
        bool: ë¡œê·¸ ê¸°ë¡ ì„±ê³µ ì—¬ë¶€
    """
    try:
        os.makedirs(FOLDERS["logs"], exist_ok=True)
        
        log_date = datetime.now().strftime('%Y%m%d')
        log_filename = os.path.join(
            FOLDERS["logs"], 
            f"upload_log_{log_date}.txt"
        )
        
        session_num = getattr(st.session_state, 'session_number', CURRENT_SESSION)
        session_label = getattr(st.session_state, 'session_label', SESSION_LABELS.get(CURRENT_SESSION, "Session 1"))
        original_nickname = getattr(st.session_state, 'original_nickname', 'Unknown')
        
        research_scores = getattr(st.session_state, 'research_scores', {})
        accuracy_score = research_scores.get('accuracy_score', 'N/A')
        fluency_score = research_scores.get('fluency_score', 'N/A')
        dual_eval_used = getattr(st.session_state, 'gpt_debug_info', {}).get('dual_evaluation', False)
        
        # ìê¸°íš¨ëŠ¥ê° í‰ê·  ê³„ì‚° (6ê°œ)
        efficacy_avg = calculate_self_efficacy_average()
        
        # ğŸ”¥ ë§¤í•‘ íŒŒì¼ ë™ê¸°í™” ìƒíƒœ í™•ì¸
        needs_upload, freshness_reason = check_mapping_file_freshness()
        mapping_sync_status = "SKIPPED (consent.py already synced)" if not needs_upload else "UPLOADED (needed refresh)"
        
        upload_status = "SUCCESS" if uploaded_files and not errors else "PARTIAL" if uploaded_files else "FAILED"
        
        log_entry = f"""
[{datetime.now().strftime(LOG_FORMAT['timestamp_format'])}] SESSION: {session_label} - {session_id}_{timestamp}
Nickname: {original_nickname}
Status: {upload_status}
Save Trigger: Auto-save after second recording completion
Dual Evaluation: {dual_eval_used} (Research scores: Accuracy={accuracy_score}, Fluency={fluency_score})
Self-Efficacy: Average {efficacy_avg}/5.0 (6 items collected)
TOPIK Scores: 3-area reference scores included in ZIP
GCS Enabled: {GCS_ENABLED} (Service Account method - ZIP-only backup)
Bucket: {GCS_BUCKET_NAME}
Files uploaded: {len(uploaded_files)} ({', '.join(uploaded_files) if uploaded_files else 'None'})
Mapping file sync: {mapping_sync_status} ({freshness_reason})
Errors: {len(errors)} ({'; '.join(errors) if errors else 'None'})
Email notification: {'Sent' if email_sent else 'Not sent/Failed'}
Data Safety: Secured before survey step
Consent Format: HTML (Korean language support)
Excel Format: TOPIK 3-area scores included in ZIP only
{'='*80}
"""
        
        with open(log_filename, 'a', encoding='utf-8') as log_file:
            log_file.write(log_entry)
        
        return True
    except Exception:
        return False


def display_download_buttons(csv_filename, excel_filename, zip_filename):
    """
    ì—°êµ¬ììš© ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ë“¤ í‘œì‹œ (TOPIK ì—‘ì…€ì€ ZIPì—ë§Œ í¬í•¨)
    
    Args:
        csv_filename: CSV íŒŒì¼ ê²½ë¡œ
        excel_filename: ì°¸ê³ ìš© ì—‘ì…€ íŒŒì¼ ê²½ë¡œ (ZIPì— í¬í•¨ë¨)
        zip_filename: ZIP íŒŒì¼ ê²½ë¡œ
    """
    if GCS_ENABLED:
        st.info(f"ğŸ“¤ Session {CURRENT_SESSION} ZIP file (with TOPIK Excel inside) should be automatically uploaded to Google Cloud Storage. Use these downloads as backup only.")
    else:
        st.warning("âš ï¸ GCS upload is disabled. Use these download buttons to save your data.")
    
    col1, col2, col3 = st.columns(3)
    
    session_num = getattr(st.session_state, 'session_number', CURRENT_SESSION)
    timestamp_str = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    with col1:
        # ZIP ì™„ì „ ë°±ì—… ë‹¤ìš´ë¡œë“œ
        if zip_filename and os.path.exists(zip_filename):
            try:
                with open(zip_filename, 'rb') as f:
                    zip_data = f.read()
                st.download_button(
                    label=f"ğŸ“¦ Session {session_num} Complete Backup ZIP",
                    data=zip_data,
                    file_name=f"session{session_num}_{st.session_state.session_id}_{timestamp_str}.zip",
                    mime='application/zip',
                    use_container_width=True
                )
            except:
                st.info("ZIP unavailable")
        else:
            st.info("ZIP unavailable")
    
    with col2:
        # CSV ë‹¤ìš´ë¡œë“œ
        if csv_filename and os.path.exists(csv_filename):
            try:
                with open(csv_filename, 'r', encoding='utf-8') as f:
                    csv_data = f.read()
                st.download_button(
                    label="ğŸ“„ CSV Data (Open in Excel)",
                    data=csv_data,
                    file_name=f"session{session_num}_{st.session_state.session_id}_{timestamp_str}.csv",
                    mime='text/csv',
                    use_container_width=True
                )
            except:
                st.error("CSV download failed")
        else:
            st.info("CSV unavailable")
    
    with col3:
        st.info("ğŸ“Š TOPIK scores inside ZIP")
    
    st.caption(f"â„¹ï¸ Session {CURRENT_SESSION} data includes self-efficacy scores, TOPIK 3-area scores (Excel inside ZIP), and consent form.")


def display_session_details():
    """
    ì—°êµ¬ììš© ì„¸ì…˜ ìƒì„¸ ì •ë³´ í‘œì‹œ (ë§¤í•‘ íŒŒì¼ ë™ê¸°í™” ì •ë³´ í¬í•¨)
    """
    st.markdown("**ğŸ“‹ Session Details:**")
    display_name = getattr(st.session_state, 'original_nickname', st.session_state.session_id)
    session_label = getattr(st.session_state, 'session_label', SESSION_LABELS.get(CURRENT_SESSION, "Session 1"))
    st.write(f"**Participant:** {display_name} (ID: {st.session_state.session_id})")
    st.write(f"**Session:** {session_label}")
    st.write(f"**Question:** {EXPERIMENT_QUESTION}")
    st.write(f"**Completed:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    st.write(f"**Data Saved:** After second recording completion")
    
    # ë°°ê²½ ì •ë³´ í‘œì‹œ
    learning_duration = getattr(st.session_state, 'learning_duration', '')
    speaking_confidence = getattr(st.session_state, 'speaking_confidence', '')
    if learning_duration:
        st.write(f"**Learning Duration:** {learning_duration}")
    if speaking_confidence:
        st.write(f"**Speaking Confidence:** {speaking_confidence}")
    
    # ìê¸°íš¨ëŠ¥ê° ì ìˆ˜ í‘œì‹œ (6ê°œ)
    efficacy_avg = calculate_self_efficacy_average()
    if efficacy_avg > 0:
        st.write(f"**Self-Efficacy:** {efficacy_avg}/5.0 (6 items)")
        with st.expander("ğŸ¯ Self-Efficacy Details", expanded=False):
            for i in range(1, 7):
                score = getattr(st.session_state, f'self_efficacy_{i}', 0)
                if score:
                    st.write(f"Item {i}: {score}/5")
    
    # ì—°êµ¬ìš© ì ìˆ˜ ì •ë³´ í‘œì‹œ
    research_scores = getattr(st.session_state, 'research_scores', {})
    if research_scores:
        st.markdown("**ğŸ”¬ Research Scores:**")
        accuracy = research_scores.get('accuracy_score', 'N/A')
        fluency = research_scores.get('fluency_score', 'N/A')
        error_rate = research_scores.get('error_rate', 'N/A')
        word_count = research_scores.get('word_count', 'N/A')
        st.write(f"Accuracy Score: {accuracy}/10 (Error rate: {error_rate}%)")
        st.write(f"Fluency Score: {fluency}/10 (Word count: {word_count})")
        dual_eval = getattr(st.session_state, 'gpt_debug_info', {}).get('dual_evaluation', False)
        st.write(f"Dual Evaluation System: {'âœ… Active' if dual_eval else 'âŒ Not used'}")
    else:
        st.write("**ğŸ”¬ Research Scores:** âŒ Not calculated")
    
    # ğŸ”¥ TOPIK ì°¸ê³ ìš© ì ìˆ˜ ì •ë³´ ì¶”ê°€
    st.markdown("**ğŸ“Š TOPIK Reference Scores:**")
    timestamp = getattr(st.session_state, 'saved_timestamp', datetime.now().strftime("%Y%m%d_%H%M%S"))
    reference_file = f"data/reference_scores_{timestamp}.xlsx"
    if os.path.exists(reference_file):
        st.write(f"âœ… TOPIK 3-area scores saved to Excel")
        st.write(f"File: reference_scores_{timestamp}.xlsx")
        st.write(f"Contains: Content/Task, Language Use, Delivery scores (1-5 each)")
        st.write(f"Location: Inside ZIP file only")
    else:
        st.write("âŒ TOPIK reference scores not found")
    
    # ğŸ”¥ ë§¤í•‘ íŒŒì¼ ë™ê¸°í™” ìƒíƒœ í‘œì‹œ
    st.markdown("**ğŸ”„ Mapping File Sync Status:**")
    needs_upload, freshness_reason = check_mapping_file_freshness()
    
    if needs_upload:
        st.warning(f"âš ï¸ Mapping file needs update: {freshness_reason}")
    else:
        st.success(f"âœ… Mapping file is current: {freshness_reason}")
    
    # GCS ì—°ë™ ìƒíƒœ í‘œì‹œ
    st.markdown("**â˜ï¸ Google Cloud Storage Status:**")
    if GCS_ENABLED:
        st.success(f"âœ… GCS upload is enabled (Service Account method)")
        if GCS_BUCKET_NAME:
            st.write(f"Bucket: {GCS_BUCKET_NAME}")
            st.write(f"Storage method: ZIP archives only")
            st.write(f"Consent format: HTML (Korean language support)")
            st.write(f"Self-efficacy: 6 items (1-5 scale) included")
            st.write(f"TOPIK scores: 3-area Excel inside ZIP")
            st.write(f"Save timing: Auto-save after 2nd recording")
            st.write(f"Mapping sync: Smart upload (avoid duplicates from consent.py)")
        else:
            st.warning("âš ï¸ No bucket specified")
        
        success, message = test_gcs_connection()
        if success:
            st.write("ğŸ”— Connection: âœ… Active")
        else:
            st.write(f"ğŸ”— Connection: âŒ {message}")
    else:
        st.warning("âŒ GCS upload is disabled")


def display_data_quality_info():
    """
    ë°ì´í„° í’ˆì§ˆ ì •ë³´ í‘œì‹œ
    """
    st.markdown("**ğŸ“Š Data Quality:**")
    col1, col2 = st.columns(2)
    
    with col1:
        duration1 = getattr(st.session_state, 'audio_duration_1', 0)
        if duration1 > 0:
            quality1 = get_quality_description(duration1)
            st.write(f"First recording: {duration1:.1f}s")
            st.caption(quality1)
        
        if st.session_state.feedback:
            issues_count = len(st.session_state.feedback.get('grammar_issues', []))
            vocab_count = len(st.session_state.feedback.get('vocabulary_suggestions', []))
            content_count = len(st.session_state.feedback.get('content_expansion_suggestions', []))
            st.write(f"Grammar issues: {issues_count}")
            st.write(f"Vocab suggestions: {vocab_count}")
            st.write(f"Content ideas: {content_count}")
            
            student_score = st.session_state.feedback.get('interview_readiness_score', 'N/A')
            st.write(f"Student UI Score: {student_score}/10")
        
        # ìê¸°íš¨ëŠ¥ê° ìš”ì•½ (6ê°œ)
        efficacy_avg = calculate_self_efficacy_average()
        if efficacy_avg > 0:
            st.write(f"**ğŸ¯ Self-Efficacy:** {efficacy_avg}/5.0")
    
    with col2:
        duration2 = getattr(st.session_state, 'audio_duration_2', 0)
        if duration2 > 0:
            quality2 = get_quality_description(duration2)
            st.write(f"Second recording: {duration2:.1f}s")
            st.caption(quality2)
        
        if hasattr(st.session_state, 'improvement_assessment'):
            improvement = st.session_state.improvement_assessment
            score = improvement.get('improvement_score', 0)
            first_score = improvement.get('first_attempt_score', 0)
            second_score = improvement.get('second_attempt_score', 0)
            st.write(f"Improvement score: {score}/10")
            st.write(f"Progress: {first_score} â†’ {second_score}")
            improvements = len(improvement.get('specific_improvements', []))
            issues = len(improvement.get('remaining_issues', []))
            st.write(f"Improvements: {improvements}")
            st.write(f"Remaining issues: {issues}")
        
        if hasattr(st.session_state, 'data_saved') and st.session_state.data_saved:
            st.write("ğŸ’¾ **Data Status:** âœ… Safely saved")
        else:
            st.write("ğŸ’¾ **Data Status:** âš ï¸ Not yet saved")


def get_quality_description(duration):
    """
    ìŒì„± ê¸¸ì´ì— ë”°ë¥¸ í’ˆì§ˆ ì„¤ëª… ë°˜í™˜ (1-2ë¶„ ëª©í‘œ ê¸°ì¤€)
    
    Args:
        duration: ìŒì„± ê¸¸ì´ (ì´ˆ)
        
    Returns:
        str: í’ˆì§ˆ ì„¤ëª…
    """
    if duration >= 90:
        return "âœ… Excellent (1.5min+ target reached!)"
    elif duration >= 75:
        return "ğŸŒŸ Good (1.25-1.5min, try for 1.5min+)"
    elif duration >= 60:
        return "âš ï¸ Fair (1-1.25min, needs improvement)"
    else:
        return "âŒ Very Short (under 1min, much more needed)"